% Notes on Statistical Learning with R
% Kevin Qian
% September, 2014

# 线性回归 #
面对一个数据集，比如Advertising，包括四个变量，sales，TV，radio，newspaper。我们要问几个问题：

1. 广告预算与销售额之间有关系吗？
2. 如果有上述关系，这种关系有多强？
3. 哪种广告媒介影响销售额？
4. 广告媒介对销售额的影响能准确估计吗？
5. 能准确预测未来的销售额吗？
6. 广告支出与销售额之间的关系是线性的吗？
7. 广告媒介之间存在协同效应吗？

## 简单线性回归 ##
假设$X$和$Y$之间近似存在线性关系：
$$Y\approx \beta_0 + \beta_1 X.$$
一旦获得了参数的估计值$\hat{\beta}_0,\hat{\beta}_1$，我们就可以根据某个特定的$X$，预测$Y$：
$$\hat{y}=\hat{\beta}_0 + \hat{\beta}_1 x,$$
其中，$\hat{y}$表示在$X=x$时$Y$的一个预测。

我们要找到最能够拟合数据集的参数估计，使得组成的直线尽可能靠近数据点。有多种方式来衡量“近”，最常用的方法之一是最小二乘法（OLS）。简单线性回归的OLS估计为：
$$\hat{\beta}_1 = \frac{\sum_{i=1}^n (x_i -\bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2},$$
$$\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x},$$
其中，$\bar{y},\bar{x}$为样本均值。

## 多元线性回归 ##
$$Y=\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p + \epsilon.$$

作多元回归，一般要回答四个问题：

1. 响应变量和自变量之间相关吗？
2. 是否所有的自变量都能解释Y，抑或只有一部分有用？
3. 模型对数据的拟合程度如何？
4. 给定自变量值，如何预测响应变量的值？预测的准确性如何？

### 问题1：响应变量与自变量之间有关系吗？ ###
对于这个问题，可以用F检验。原假设为所有的参数均为0：
$$ H_0:\beta_1 = \beta_2 = \cdots = \beta_p = 0;\quad H_a: \exists j,\beta_j\neq 0.$$
构造F统计量：
$$F=\frac{(\mathrm{TSS}-\mathrm{RSS})/p}{\mathrm{RSS}/(n-p-1)},$$
可以证明，当原假设为真时，上述统计量中分子和分母的期望值均为$\sigma^2$（误差方差），因此，如果自变量和响应变量不存在关系，F统计量应该接近1。到底F多大时，可以拒绝原假设？这要看$n$和$p$的大小。当$n$较大时，即使F统计量只比1稍大也可以拒绝原假设。反之，当$n$比较小时，需要更大的F值。当$H_0$为真且误差项服从正态分布时，F统计量服从F分布。由此，可以从相应的p值来判断是否可以拒绝原假设。

（TODO：F检验与t检验的关系，为何需要F检验？）

### 问题2：哪些变量是重要的？###
多元回归分析的第一步，是计算F统计量并检查相应的p值。如果这一步得出至少有一个自变量与响应变量有关，那么很自然地，我们要找出哪些是重要的（应该进入模型），这项任务称为“变量选择”。我们可以查看每一个变量的p值（t检验），但是如果自变量数量很多（p很大），这种办法会导致错误。

通过选取不同的变量，可以生成$2^p$个模型。当$p$较大时，尝试所有的模型显然是不切实际的。我们需要一种自动且有效的方法来选取一部分模型进行考虑。有三种传统办法：

* 前向选择。
* 后向选择。当$p>n$时不可用。
* 混合选择。

### 问题3：模型拟合性 ###
最常用的两个衡量拟合性的数值是RSE和$R^2$。一元回归中，$R^2=\mathrm{Cov}(X,Y)^2$；在多元线性回归中，$R^2=\mathrm{Cov}(Y,\hat{Y})^2$。事实上，拟合线性模型的一个性质就是，使这个相关系数在所有线性模型中最大。

除了看RSE和R方，还可以通过图形来观察。在销售额的例子中，如果作出sales ~ TV + radio的三维图，从残差的分布情况可以看到，变量之间存在非线性，意味着可能需要加入交互项（协同效应）。

### 问题4：预测 ###
利用回归得到的参数估计来预测，会面临三类不确定性：

1. 可减少误差。来自OLS回归平面$\hat{Y}$与真实总体回归平面$f(X)$之间的差距，可以由置信区间来表示。
2. 模型偏差。来自线性模型与实际不符产生的误差，这种误差也是可减少的。
3. 不可减少误差。即使我们知道真实的参数，也会面临由随机误差项带来的不可减少的误差。也就是$Y$与$\hat{Y}$的差距。可以用预测区间来表示。

（TODO：预测区间与置信区间的区别？）

## 回归模型中的其他问题 ##

### 定性预测子 ###
定性变量一般称为因子（factor），其可能的取值称为水平（level）。对于仅有两个水平的因子，将其加入模型是非常简单的，只要构造一个哑变量。比如，对于gender变量，可以构造一个新的变量$x_i$，当第$i$个人为女性时取1，反之取0。相应的回归模型可以写成：

$$
\begin{eqnarray}
y_i = \beta_0 + \beta_1 x_i + \epsilon_i =
\begin{cases}
\beta_0 + \beta_1 + \epsilon_i       & \mathrm{第}i\mathrm{个为女性} \\
\beta_0 + \epsilon_i       & \mathrm{第}i\mathrm{个为男性}
\end{cases}
\end{eqnarray}
$$
回归系数的解释：
对于取三个可能值的变量，可以定义两个亚变量，模型参数的解释类似。定性变量的编码方式不唯一，但不同方式都将得到同样的模型拟合结果，只是系数及其解释不同（都表示某种比较）。

### 线性模型的拓展 ###
标准线性回归模型带有很多假设，其中涉及自变量和应变量关系的两个重要假设是可加性（additive）和线性（linear）。

我们可以通过加入交互项放松可加性条件。以Advertising数据集为例，可加入radio $\times$ TV。有时，加入的交互项显著，而主效应却不显著。此时，要不要包括主效应呢？统计学习中有如下原理。

**层次原理**（hierarchical principle）：如果模型中包括交互项，即使主效应对应的p值不显著，也必须包括在模型中。

交互项可以包含定量和定性的变量。比如Credit数据集，设想我们希望通过income和student预测balance。当不包括交互项时，模型可以表示为两条平行线，income对balance的平均影响与是不是学生无关。这样的模型是有局限的，解决办法是加入交互项。加入收入和学生的交互项后，可以得到两条回归线，截距和斜率均不同。

体现非线性关系的一种简单形式是多项式回归。

### 潜在问题 ###
拟合线性回归模型时常常会碰到如下问题：

1. 非线性关系。
2. 误差项相关。
3. 异方差。
4. 异常值。
5. 高杠杆值。
6. 共线性。

**非线性关系**。用残差对于拟合值作残差图，如果模型是线性的，残差图不应该呈现明显的模式。如果残差图显示有非线性特征，可以对自变量进行简单非线性变换，比如$\log X$，$\sqrt{X}$或$X^2$等。当然，还有更加高级的处理非线性问题的方法。

**误差项相关**。如果误差项相关，那么标准误差将被低估，带来的结果是，置信区间和预测区间都将变窄。比如，95%的置信区间实际包含真实参数值的概率要比0.95小得多。另外，模型对应的p值也将变小，这将导致误以为参数是显著的。最常见的例子是时间序列。

**异方差**。当误差项的方差不同时，标准误、置信区间和相应的假设检验都将不靠谱。鉴别异方差的一种方法是看残差图是不是出现所谓的漏斗状（funnel shape）。出现这种情况，一种解决办法是对应变量$Y$进行对数或开方等非线性变换，从而大大缩小$Y$的值并减少异方差。有时，我们对每一个应变量的方差有较为明确的认识，这时可以采用某种形式的加权最小二乘法（WLS）。

**异常值**。所谓异常值，是指$y_i$与其预测值差距很大的数据点。异常值一般对模型拟合影响不大，但会对RSE、R方产生影响。残差图可以探测异常值，但实践中通常残差大小难以决断，这时可以用t化残差来判断，一般t化残差绝对值大于3的观测可能是异常值。模型分析中是否要移除异常值，需要谨慎考虑。

**高杠杆值**。与异常值关注$y_i$不同，高杠杆值是看$x_i$是否不同寻常。如果一个观测的自变量比其他观测明显要大时，可能就是高杠杆值。相比异常值，高杠杆值对模型拟合有较大影响。衡量一个观测的杠杆度，一般用杠杆统计量，对于简单线性回归，杠杆统计量由下式计算：
$$h_i = \frac{1}{n} + \frac{(x_i - \bar{x})^2} {\sum_{i'=1}^n (x_{i'} - \bar{x})^2}.$$
显然，如果$x_i$与$\bar{x}$差异越大，$h_i$越大。此外，杠杆值必定位于$1/n$和1之间，所有观测的平均杠杆值等于$(p+1)/n$。因此，当某个观测的杠杆值超过$(p+1)/n$较多时，我们有理由怀疑它是高杠杆值。如果一个观测既是异常值，又是高杠杆值，是非常危险的组合，需要高度重视！

**共线性**。如果模型中某些自变量之间相关时，回归系数的准确性会降低，对应的标准误会增大，从而t统计量会减小，结果导致该拒绝原假设时没拒绝，也就是降低了假设检验的power。一个检测共线性的简单办法是看自变量的相关矩阵，其中一个较大的值表示变量之间相关性较高。但是，相关矩阵只能看两个变量的关系，而共线性可能存在与三个及以上变量之间，也就是多重共线性。此时，更好的办法是计算方差膨胀因子（VIF）。计算如下： 
$$\mathrm{VIF}(\hat{\beta}_j) = \frac{1}{1-R^2_{X_j|X_{-j}}}.$$
其中，$R^2_{X_j|X_{-j}}$是$X_j$对其他所有自变量进行回归得到的R方。如果$R^2_{X_j|X_{-j}}$接近1，存在共线性，此时VIF很大。面临共线性问题，有两种常用解决办法，一种是去掉一个问题变量，另一种是将共线性的变量合并成一个变量（比如标准化后取平均）。

## 市场营销方案 ##
现在可以用多元回归分析的结果回答开头提出的七个问题。

## 线性回归与*K*-近邻 ##
线性回归属于参数方法，优点易于拟合和解释，缺点是容易脱离实际，导致预测不准。非参数方法也可用于回归，一个简单著名的例子是KNN回归。KNN回归与KNN分类器密切相关。给定$K$和预测点$x_0$，KNN回归首先找到与$x_0$最接近的$K$个训练点，组成集合$\mathbb{N}_0$，接着，用这些训练点的响应值的均值来估计$f(x_0)$：
$$\hat{f}(x_0) = \frac{1}{K}\sum_{x_i\in \mathbb{N}_0} y_i.$$

选择多大的$K$，取决于偏差与方差之间的权衡。$K$越小，模型越flexible，偏差越小，但同时方差也越大；反之则反之。后面会讨论一些计算测试误差率的方法，这些方法可用于$K$的选择。

到底是选择参数化的线性模型还是非参数方法（比如KNN）呢？一个简单的原则是，当实际模型接近线性时，就采用线性模型。可以证明，对于变量个数较少的情况，当模型的非线性程度增加时，非参数方法的test MSE只有微小变化，而参数化线性模型的test MSE却大大增加。换句话说，当真实关系为线性时，KNN比线性模型稍差；但当真实关系为非线性时，KNN却比线性模型好很多。这个特征会导致一种倾向，就是“KNN应该优先选用”，然而，实际上这并不对。主要问题在于，当模型维数（自变量个数）增加时，即使真实关系为非线性，KNN的表现也会比线性模型糟糕得多。原因就是著名的“**维数灾难**”。事实上，即使在p较小时，我们一般也会由于便于解释等原因而倾向于选用线性模型（愿意牺牲一点预测准确性）。

# 分类 #
当响应变量为定性（类别）变量时，线性回归就不适用了。此时要用专门的分类方法。

## Logistic回归 ##

## 线性判别分析 ##

## K-近邻 ##
