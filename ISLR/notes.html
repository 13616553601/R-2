<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Kevin Qian" />
  <title>统计学习笔记</title>
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="/Users/kevin/Bak/styles/style.css" type="text/css" />
  <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<div id="header">
<h1 class="title">统计学习笔记</h1>
<h2 class="author">Kevin Qian</h2>
<h3 class="date">二零一四年九月</h3>
</div>
<h1 id="线性回归">线性回归</h1>
<p>面对一个数据集，比如Advertising，包括四个变量，sales，TV，radio，newspaper。我们要问几个问题：</p>
<ol style="list-style-type: decimal">
<li>广告预算与销售额之间有关系吗？</li>
<li>如果有上述关系，这种关系有多强？</li>
<li>哪种广告媒介影响销售额？</li>
<li>广告媒介对销售额的影响能准确估计吗？</li>
<li>能准确预测未来的销售额吗？</li>
<li>广告支出与销售额之间的关系是线性的吗？</li>
<li>广告媒介之间存在协同效应吗？</li>
</ol>
<h2 id="简单线性回归">简单线性回归</h2>
<p>假设<span class="math">\(X\)</span>和<span class="math">\(Y\)</span>之间近似存在线性关系： <span class="math">\[Y\approx \beta_0 + \beta_1 X.\]</span> 一旦获得了参数的估计值<span class="math">\(\hat{\beta}_0,\hat{\beta}_1\)</span>，我们就可以根据某个特定的<span class="math">\(X\)</span>，预测<span class="math">\(Y\)</span>： <span class="math">\[\hat{y}=\hat{\beta}_0 + \hat{\beta}_1 x,\]</span> 其中，<span class="math">\(\hat{y}\)</span>表示在<span class="math">\(X=x\)</span>时<span class="math">\(Y\)</span>的一个预测。</p>
<p>我们要找到最能够拟合数据集的参数估计，使得组成的直线尽可能靠近数据点。有多种方式来衡量“近”，最常用的方法之一是最小二乘法（OLS）。简单线性回归的OLS估计为： <span class="math">\[\hat{\beta}_1 = \frac{\sum_{i=1}^n (x_i -\bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2},\]</span> <span class="math">\[\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x},\]</span> 其中，<span class="math">\(\bar{y},\bar{x}\)</span>为样本均值。</p>
<h2 id="多元线性回归">多元线性回归</h2>
<p><span class="math">\[Y=\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p + \epsilon.\]</span></p>
<p>作多元回归，一般要回答四个问题：</p>
<ol style="list-style-type: decimal">
<li>响应变量和自变量之间相关吗？</li>
<li>是否所有的自变量都能解释Y，抑或只有一部分有用？</li>
<li>模型对数据的拟合程度如何？</li>
<li>给定自变量值，如何预测响应变量的值？预测的准确性如何？</li>
</ol>
<h3 id="问题1响应变量与自变量之间有关系吗">问题1：响应变量与自变量之间有关系吗？</h3>
<p>对于这个问题，可以用F检验。原假设为所有的参数均为0： <span class="math">\[ H_0:\beta_1 = \beta_2 = \cdots = \beta_p = 0;\quad H_a: \exists j,\beta_j\neq 0.\]</span> 构造F统计量： <span class="math">\[F=\frac{(\mathrm{TSS}-\mathrm{RSS})/p}{\mathrm{RSS}/(n-p-1)},\]</span> 可以证明，当原假设为真时，上述统计量中分子和分母的期望值均为<span class="math">\(\sigma^2\)</span>（误差方差），因此，如果自变量和响应变量不存在关系，F统计量应该接近1。到底F多大时，可以拒绝原假设？这要看<span class="math">\(n\)</span>和<span class="math">\(p\)</span>的大小。当<span class="math">\(n\)</span>较大时，即使F统计量只比1稍大也可以拒绝原假设。反之，当<span class="math">\(n\)</span>比较小时，需要更大的F值。当<span class="math">\(H_0\)</span>为真且误差项服从正态分布时，F统计量服从F分布。由此，可以从相应的p值来判断是否可以拒绝原假设。</p>
<p>（TODO：F检验与t检验的关系，为何需要F检验？）</p>
<h3 id="问题2哪些变量是重要的">问题2：哪些变量是重要的？</h3>
<p>多元回归分析的第一步，是计算F统计量并检查相应的p值。如果这一步得出至少有一个自变量与响应变量有关，那么很自然地，我们要找出哪些是重要的（应该进入模型），这项任务称为“变量选择”。我们可以查看每一个变量的p值（t检验），但是如果自变量数量很多（p很大），这种办法会导致错误。</p>
<p>通过选取不同的变量，可以生成<span class="math">\(2^p\)</span>个模型。当<span class="math">\(p\)</span>较大时，尝试所有的模型显然是不切实际的。我们需要一种自动且有效的方法来选取一部分模型进行考虑。有三种传统办法：</p>
<ul>
<li>前向选择。</li>
<li>后向选择。当<span class="math">\(p&gt;n\)</span>时不可用。</li>
<li>混合选择。</li>
</ul>
<h3 id="问题3模型拟合性">问题3：模型拟合性</h3>
<p>最常用的两个衡量拟合性的数值是RSE和<span class="math">\(R^2\)</span>。一元回归中，<span class="math">\(R^2=\mathrm{Cov}(X,Y)^2\)</span>；在多元线性回归中，<span class="math">\(R^2=\mathrm{Cov}(Y,\hat{Y})^2\)</span>。事实上，拟合线性模型的一个性质就是，使这个相关系数在所有线性模型中最大。</p>
<p>除了看RSE和R方，还可以通过图形来观察。在销售额的例子中，如果作出sales ~ TV + radio的三维图，从残差的分布情况可以看到，变量之间存在非线性，意味着可能需要加入交互项（协同效应）。</p>
<h3 id="问题4预测">问题4：预测</h3>
<p>利用回归得到的参数估计来预测，会面临三类不确定性：</p>
<ol style="list-style-type: decimal">
<li>可减少误差。来自OLS回归平面<span class="math">\(\hat{Y}\)</span>与真实总体回归平面<span class="math">\(f(X)\)</span>之间的差距，可以由置信区间来表示。</li>
<li>模型偏差。来自线性模型与实际不符产生的误差，这种误差也是可减少的。</li>
<li>不可减少误差。即使我们知道真实的参数，也会面临由随机误差项带来的不可减少的误差。也就是<span class="math">\(Y\)</span>与<span class="math">\(\hat{Y}\)</span>的差距。可以用预测区间来表示。</li>
</ol>
<p>（TODO：预测区间与置信区间的区别？）</p>
<h2 id="回归模型中的其他问题">回归模型中的其他问题</h2>
<h3 id="定性预测子">定性预测子</h3>
<p>定性变量一般称为因子（factor），其可能的取值称为水平（level）。对于仅有两个水平的因子，将其加入模型是非常简单的，只要构造一个哑变量。比如，对于gender变量，可以构造一个新的变量<span class="math">\(x_i\)</span>，当第<span class="math">\(i\)</span>个人为女性时取1，反之取0。相应的回归模型可以写成：</p>
<p><span class="math">\[
\begin{eqnarray}
y_i = \beta_0 + \beta_1 x_i + \epsilon_i =
\begin{cases}
\beta_0 + \beta_1 + \epsilon_i       &amp; \mathrm{第i个为女性} \\
\beta_0 + \epsilon_i       &amp; \mathrm{第i个为男性}
\end{cases}
\end{eqnarray}
\]</span> 回归系数的解释： 对于取三个可能值的变量，可以定义两个亚变量，模型参数的解释类似。定性变量的编码方式不唯一，但不同方式都将得到同样的模型拟合结果，只是系数及其解释不同（都表示某种比较）。</p>
<h3 id="线性模型的拓展">线性模型的拓展</h3>
<p>标准线性回归模型带有很多假设，其中涉及自变量和应变量关系的两个重要假设是可加性（additive）和线性（linear）。</p>
<p>可加性条件的放松，常见是加入交互项，以<span class="math">\(\mathtt{Advertising}\)</span>数据集为例，可加入<span class="math">\(\mathtt{radio}\times \mathtt{TV}\)</span>。</p>
<pre><code>层次原理（hierarchical principle）：如果模型中包括交互项，即使主效应对应的p值不显著，也必须包括在模型中。</code></pre>
<p>交互项可以包含定量和定性的变量。比如<span class="math">\(\mathtt{Credit}\)</span>数据集，设想我们希望通过<span class="math">\(\mathtt{income}\)</span>和<span class="math">\(\mathtt{student}\)</span>预测<span class="math">\(\mathtt{balance}\)</span>。当不包括交互项时，模型可以表示为两条平行线，<span class="math">\(\mathtt{income}\)</span>对<span class="math">\(\mathtt{balance}\)</span>的平均影响与是不是学生无关。这样的模型是有局限的，解决办法是加入交互项。加入收入和学生的交互项后，可以得到两条回归线，截距和斜率均不同。</p>
</body>
</html>
